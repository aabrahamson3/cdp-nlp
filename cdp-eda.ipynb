{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from cdptools import CDPInstance, configs\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_to_dataframe(transcript):\n",
    "    # Create rows container and counter\n",
    "    rows = []\n",
    "    current_overall_sentence_index = 0\n",
    "    \n",
    "    # Iterate through transcripts\n",
    "    for i, speaker_block in enumerate(transcript[\"data\"]):\n",
    "        for j, sentence in enumerate(speaker_block[\"data\"]):\n",
    "            rows.append({\n",
    "                \"speaker_block_index\": i,\n",
    "                \"speaker_sentence_index\": j,\n",
    "                \"overall_sentence_index\": current_overall_sentence_index,\n",
    "                **sentence\n",
    "            })\n",
    "            current_overall_sentence_index += 1\n",
    "            \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download vader lexicon, and instatiate a SentimentIntensityAnalyzer object\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sid = SentimentIntensityAnalyzer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_compound_score(sent):\n",
    "    # returns just the compound sentiment VADER score\n",
    "    ss = sid.polarity_scores(sent)\n",
    "    out = ss['compound']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_score_whole_transcript(transcript):\n",
    "    \"\"\"\n",
    "    takes in a list of text from a transcript and scores each sentence with NLTK VADER,\n",
    "    returns list of VADER Compund Score\n",
    "    \"\"\"\n",
    "    scored = []\n",
    "    for i, row in transcript.iterrows():\n",
    "        row = dict(row)\n",
    "        scored.append({\n",
    "            **row,\n",
    "            \"score\": get_vader_compound_score(row[\"text\"])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to CDP database\n",
    "seattle = CDPInstance(configs.SEATTLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a specific event, save the json\n",
    "manifest = seattle.get_transcript_manifest()\n",
    "manifest = manifest.loc[manifest.confidence == 0.97]\n",
    "\n",
    "# Iter generate plots\n",
    "for i, row in manifest.iterrows():\n",
    "    save_path = seattle.file_store.download_file(row.filename, overwrite=True)\n",
    "\n",
    "    # Open and read transcript\n",
    "    with open(save_path, \"r\") as read_in:\n",
    "        raw_transcript = json.load(read_in)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    transcript = transcript_to_dataframe(raw_transcript)\n",
    "    # Generate scores for each sentence\n",
    "    transcript = vader_score_whole_transcript(transcript)\n",
    "\n",
    "    # Generate speaker blocks\n",
    "    speaker_blocks = transcript.groupby(\"speaker_block_index\")\n",
    "    speaker_block_averaged_rows = []\n",
    "    for speaker_block_index, row_indicies in speaker_blocks.groups.items():\n",
    "        # Get rows\n",
    "        speaker_rows = transcript.loc[row_indicies]\n",
    "\n",
    "        # Create speaker block row\n",
    "        speaker_block_averaged_rows.append({\n",
    "            \"speaker_block_index\": speaker_block_index,\n",
    "            \"start_time\": transcript.loc[row_indicies[0]].start_time,\n",
    "            \"end_time\": transcript.loc[row_indicies[0]].end_time,\n",
    "            \"average_score\": speaker_rows.score.mean(),\n",
    "            \"median_score\": speaker_rows.score.median(),\n",
    "        })\n",
    "\n",
    "    speaker_block_averaged_rows = pd.DataFrame(speaker_block_averaged_rows)\n",
    "    \n",
    "    # Generate chart\n",
    "    alt.Chart(speaker_block_averaged_rows).mark_line(interpolate=\"basis\").encode(\n",
    "        x=\"speaker_block_index\",\n",
    "        y=\"average_score\",   \n",
    "    ).save(f\"{row.event_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
